{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7Z8eeN5IW9q"
   },
   "source": [
    "The deadline is 9:30am Feb 9th (Wed).   \n",
    "You should submit a `.ipynb` file with your solutions to BrightSpace.\n",
    "\n",
    "--- \n",
    "\n",
    "There are 10 extra points for \"adding extra features to your model\". But the maximum grade you can obtain in this homework is 100%. If you complete the extra-credit task, your score will be min{10+score, 100}.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this homework we will preprocess SMS Spam Collection Dataset and train a bag-of-words classifier (logistic regression) for spam detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZd0LJzbISPd"
   },
   "source": [
    "## Data Loading (10 points)\n",
    "\n",
    "First, we download the SMS Spam Collection Dataset. The dataset is taken from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data#) and loaded to [Google Drive](https://drive.google.com/open?id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR) so that everyone can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PvGErs2oHkWU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RcHV1lUwtH-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSGA1012_HW1.ipynb spam.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXVQCF-ovo4G"
   },
   "source": [
    "There are two columns: `v1` -- spam or ham indicator, `v2` -- text of the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BiKE89v0zMiY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v1                                                 v2\n",
       "0   0  Go until jurong point, crazy.. Available only ...\n",
       "1   0                      Ok lar... Joking wif u oni...\n",
       "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   0  U dun say so early hor... U c already then say...\n",
       "4   0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
    "# 1 - spam, 0 - ham\n",
    "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                Will ÃŒ_ b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: v2, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXQhTzrCv-Nk"
   },
   "source": [
    "Your task is to split the data to train/dev/test (don't forget to shuffle the data). Make sure that each row appears only in one of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ga5Qydpw-gdQ"
   },
   "outputs": [],
   "source": [
    "# 0.15 for val, 0.15 for test, 0.7 for train\n",
    "val_size = int(df.shape[0] * 0.15)\n",
    "test_size = int(df.shape[0] * 0.15)\n",
    "\n",
    "# My Code Starts Here\n",
    "df = df.sample(frac=1) #Shuffle The dataframe to achieve randomness\n",
    "df.reset_index(drop=True, inplace=True) #Reset index\n",
    "\n",
    "# Split into train/val/test sets, since we shuffled the df\n",
    "# And we are not sampling with replacement, we use a simple indexing approach\n",
    "train_texts, train_labels = df.iloc[val_size+test_size:,1], df.iloc[val_size+test_size:,0]\n",
    "val_texts, val_labels     = df.iloc[:val_size,1], df.iloc[:val_size,0]\n",
    "test_texts, test_labels   = df.iloc[val_size:val_size + test_size,1], df.iloc[val_size:val_size+test_size,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGyHG4lBISP2"
   },
   "source": [
    "## Data Processing (40 points)\n",
    "\n",
    "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. In the lab we use built-in `sklearn` function, `sklearn.feature_extraction.text.CountVectorizer`. \n",
    "**In this HW, you are required to implement the `Vectorizer` on your own without using `sklearn` built-in functions.**\n",
    "\n",
    "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
    "You may use [spacy](https://spacy.io/) or [nltk](https://www.nltk.org/) text processing libraries in `preprocess_data` function. \n",
    "\n",
    "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "793EFaQYhHeR"
   },
   "outputs": [],
   "source": [
    "#Import necessary modules for preprocessing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # This function should return a list of lists of preprocessed tokens for each message\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    data (string) - A string representing a sentence\n",
    "    \n",
    "    Output:\n",
    "    preprocessed_data (list of lists) - lists of tokens indexed in a list \n",
    "    \"\"\"\n",
    "    #Initialize return list\n",
    "    preprocessed_data = data\n",
    "    \n",
    "    #Make all sentences lowercase\n",
    "    preprocessed_data = preprocessed_data.apply(lambda X: str.lower(X))\n",
    "    \n",
    "    #Strip Commas, Periods, Ect. And tokenize each word\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    preprocessed_data = preprocessed_data.apply(tokenizer.tokenize)\n",
    "    \n",
    "    #Return our preprocessed data\n",
    "    return preprocessed_data\n",
    "\n",
    "train_data = preprocess_data(train_texts)\n",
    "val_data = preprocess_data(val_texts)\n",
    "test_data = preprocess_data(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TM2qpOKpjVbD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Vectorizer():\n",
    "    def __init__(self, max_features):\n",
    "        self.max_features = max_features\n",
    "        self.vocab_list = None\n",
    "        self.token_to_index = None\n",
    "        \n",
    "    def fit(self, dataset):\n",
    "        # Create a vocab list, self.vocab_list, using the most frequent \"max_features\" tokens\n",
    "        # Create a token indexer, self.token_to_index, that will map each token in self.vocab \n",
    "        # to its corresponding index in self.vocab_list\n",
    "        \"\"\"\n",
    "        input: \n",
    "        dataset: a preprocessed dataset of \n",
    "        \n",
    "        modifications:\n",
    "        self.vocab_list: The vocab list will now contain the first \"max_features\" of most frequent tokens\n",
    "        self.token_to_index: Maps each token in self.vocab to its index in self.vocab_list\n",
    "        \n",
    "        output:\n",
    "        True: just to let you know the function ran ok :) \n",
    "        \"\"\"\n",
    "        #Initialize a list, a dictionary, and a helper variable\n",
    "        self.vocab_list = []\n",
    "        token_counter_dict = {}\n",
    "        self.token_to_index = {}\n",
    "        \n",
    "        #Iterate over all tokens in each row\n",
    "        for row in dataset:\n",
    "            for token in row:\n",
    "                #Increment counter in dictionary: current count + 1\n",
    "                token_counter_dict[token] = token_counter_dict.get(token, 0) + 1\n",
    "                \n",
    "        #Sort the tokens by frequency\n",
    "        token_counter_dict = dict(sorted(token_counter_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        self.temp = token_counter_dict\n",
    "        \n",
    "        #Since its sorted from highest to lowest, grab the first max_features worth of tokens\n",
    "        counter = 0 #Helper variable\n",
    "        \n",
    "        #Iterate over the sorted tokens\n",
    "        for token, count in token_counter_dict.items():\n",
    "        \n",
    "            #Only append max_features number of tokens\n",
    "            if counter < max_features:\n",
    "                #Append token to vocab_list\n",
    "                self.vocab_list.append(token)\n",
    "                #Update our token_to_index dictionary to map added token to an index\n",
    "                self.token_to_index[token] = counter\n",
    "                counter += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        # This function transforms text dataset into a matrix, data_matrix\n",
    "        \"\"\"\n",
    "        input:\n",
    "        dataset: preprocessed text represented as a list of lists (tokens)\n",
    "        \n",
    "        output:\n",
    "        data_matrix: a 2D (i,j) binary-array where 1 represents the word is present in the ith row of data\n",
    "        for the jth item in self.vocab_list\n",
    "        \"\"\"\n",
    "        ### Given Code:\n",
    "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
    "        \n",
    "        ### My Code:\n",
    "        #Iterate over all the tokens in each row\n",
    "        for i, row in enumerate(dataset):\n",
    "            for token in row:\n",
    "                    # If the token is present in self.vocab_list, then include in the data_matrix\n",
    "                    if token in self.vocab_list:\n",
    "                        data_matrix[i,self.token_to_index[token]] = 1\n",
    "                        \n",
    "        #Return our fitted data matrix\n",
    "        return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wXMrZXlZjcH7"
   },
   "outputs": [],
   "source": [
    "max_features = 100 # TODO: Replace None with a number\n",
    "vectorizer = Vectorizer(max_features=max_features)\n",
    "vectorizer.fit(train_data)\n",
    "X_train = vectorizer.transform(train_data)\n",
    "X_val = vectorizer.transform(val_data)\n",
    "X_test = vectorizer.transform(test_data)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "vocab = vectorizer.vocab_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGLg6udky1zo"
   },
   "source": [
    "(10 extra points) You can add more features to the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s80GgEm6F5DG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYOUR CODE GOES HERE\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "YOUR CODE GOES HERE\n",
    "\"\"\"\n",
    "#Maybe add a 2 features: a counter for positive and a counter for negative words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wtm7a6JWu9-3"
   },
   "source": [
    "## Model\n",
    "\n",
    "We train logistic regression model and save prediction for train, val and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Wq9stSAbAIZe"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define Logistic Regression model\n",
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# Fit the model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction using the trained model\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j-Abw7JOqD_"
   },
   "source": [
    "## Performance of the model (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akg9LvP5DGE8"
   },
   "source": [
    "Your task is to report train, val, test accuracies and F1 scores. **You are required to implement `accuracy_score` and `f1_score` methods without using built-in python functions.** \n",
    "\n",
    "Your model should achieve at least **0.95** test accuracy and **0.90** test F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "chqVbKH6kZyY"
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred): \n",
    "    # Calculate accuracy of the model's prediction\n",
    "    \"\"\"\n",
    "    YOUR CODE GOES HERE\n",
    "    \"\"\"\n",
    "    accuracy = None\n",
    "    return accuracy\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    # Calculate F1 score of the model's prediction\n",
    "    \"\"\"\n",
    "    YOUR CODE GOES HERE\n",
    "    \"\"\"\n",
    "    f1 = None\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MqrMw0udDD04"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rk/rwsr6gss0vz3g4fz3_kt0x0m0000gn/T/ipykernel_1587/3032728811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n\u001b[0m\u001b[1;32m      2\u001b[0m       f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n\u001b[1;32m      3\u001b[0m print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n\u001b[1;32m      4\u001b[0m       f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n\u001b[1;32m      5\u001b[0m print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n",
    "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW7P84giGgP4"
   },
   "source": [
    "**Question.**\n",
    "Is accuracy the metric that logistic regression optimizes while training? If no, which metric is optimized in logistic regression?\n",
    "\n",
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak0h71krLPqX"
   },
   "source": [
    "**Question.**\n",
    "In general, does having 0.99 accuracy on test means that the model is great? If no, can you give an example of a case when the accuracy is high but the model is not good? (Hint: why do we use F1 score?)\n",
    "\n",
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_RDI0qdOxwM"
   },
   "source": [
    "### Exploration of predicitons (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHR2OqYCDOxs"
   },
   "source": [
    "Show a few examples with true+predicted labels on the train and val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yv8GD-UGXvR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR CODE GOES HERE\n",
    "\"\"\"\n",
    "# 1 - spam, 0 - ham\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neMQ4VR9GVL3"
   },
   "source": [
    "**Question** Print 10 examples from val set which were labeled incorrectly by the model. Why do you think the model got them wrong?\n",
    "\n",
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ssK0jRxGY3u"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR CODE GOES HERE\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSGA1012-HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
