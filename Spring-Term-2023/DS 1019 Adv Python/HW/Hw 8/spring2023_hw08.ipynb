{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 08:  Parallel Programming 01\n",
    "\n",
    "## Due Date: Apr 12, 2023, 11:59pm\n",
    "\n",
    "#### Firstname Lastname: Giulio Duregon\n",
    "\n",
    "#### E-mail: gjd9961@nyu.edu\n",
    "\n",
    "#### Enter your solutions and submit this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1 (50p)**\n",
    "\n",
    "Write an MPI program `sol08pr01.py` that does the following for some arbitrary number of processes $N \\geq 2$. Here the number of processes $N$ is given as `N` while calling the code `sol08pr01.py` as: \n",
    "\n",
    "`mpirun -n N python3 sol08pr01.py`\n",
    "\n",
    "\n",
    "Every process will contain one buffer with one integer variable, each of which is initialized to $0$.\n",
    "\n",
    "For $r=0, 1, \\dots, N - 1$, Process $r$ updates its buffer to the value received by $r-1$ (this should only be done for $r \\geq 1$), then it squares its rank $r$, adds the result $r^2$ to the value of its own buffer, and then sends the sum to Process $r + 1$. Note that for $r=N-1$ the result will be sent to Process $0$, i.e. by convention, Process $N$ is the same as Process $0$. At the end Process $0$ prints the received value. \n",
    "\n",
    "Provide results for: $N = 10$, $N = 15$, $N = 20$, $N = 25$. These are probably more than the available processes on your machine: you can use the option `--oversubscribe` in `mpirun` to let MPI run things anyway.\n",
    "\n",
    "\n",
    "\n",
    "**Note**: You can use either blocking or non-blocking operations.Make sure to provide adequate comments and documentation in the code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sol08pr01.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sol08pr01.py\n",
    "#Imports\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize comm network, get rank so processes can identify themselves / execute as appropriate\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "info = MPI.Status()\n",
    "\n",
    "# Get total number of processes\n",
    "size = comm.Get_size() \n",
    "last_rank = size-1\n",
    "\n",
    "# Initialize buffer\n",
    "buffer = np.zeros(1, dtype=int)\n",
    "\n",
    "# Start of with the first process rank == 0\n",
    "if rank == 0:\n",
    "    # print(f\"Process {rank}, sending number: {buffer[0]}, to Process {rank+1}\")\n",
    "    comm.Send(buffer, dest=(rank+1))\n",
    "    \n",
    "    # Wait for message from last process\n",
    "    # print(f\"Waiting for value from Process {size-1}\")\n",
    "    comm.Recv(buffer, source = (last_rank))\n",
    "    \n",
    "    # Print final output\n",
    "    print(f\"Received Final Number From Process {last_rank}: {buffer[0]}\")\n",
    "    \n",
    "# Do all for N-2\n",
    "if rank != last_rank and rank != 0:\n",
    "    # Receive value into buffer\n",
    "    comm.Recv(buffer, source=(rank-1), status=info)\n",
    "    # print(f\"Process {rank}, received the number {buffer[0]}, from Process {info.Get_source()}\")\n",
    "    # Square rank and add to buffer\n",
    "    buffer += rank**2\n",
    "    \n",
    "    # Send value\n",
    "    comm.Send(buffer, dest=(rank+1))\n",
    "    \n",
    "# Check for the final process (N-1)\n",
    "if rank == last_rank:\n",
    "    comm.Recv(buffer, source=(last_rank-1), status=info)\n",
    "    # print(f\"Process {rank}, received the number {buffer[0]}, from Process {info.Get_source()}\")\n",
    "    # Square rank and add to buffer\n",
    "    buffer += rank**2\n",
    "    \n",
    "    # Send value, but this time to process 0\n",
    "    print(\"Sending result back to Process 0\")\n",
    "    comm.Send(buffer, dest=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received Final Number From Process 9: 285\n",
      "Sending result back to Process 0\n",
      "--------------------------------------------------------------------------\n",
      "A system call failed during shared memory initialization that should\n",
      "not have.  It is likely that your MPI job will now either abort or\n",
      "experience performance degradation.\n",
      "\n",
      "  Local host:  giulios-mbp.lan\n",
      "  System call: unlink(2) /var/folders/rk/rwsr6gss0vz3g4fz3_kt0x0m0000gn/T//ompi.giulios-mbp.501/pid.4602/1/vader_segment.giulios-mbp.501.5b790001.9\n",
      "  Error:       No such file or directory (errno 2)\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!mpiexec --oversubscribe -n 10 python sol08pr01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending result back to Process 0\n",
      "Received Final Number From Process 14: 1015\n",
      "--------------------------------------------------------------------------\n",
      "A system call failed during shared memory initialization that should\n",
      "not have.  It is likely that your MPI job will now either abort or\n",
      "experience performance degradation.\n",
      "\n",
      "  Local host:  giulios-mbp.lan\n",
      "  System call: unlink(2) /var/folders/rk/rwsr6gss0vz3g4fz3_kt0x0m0000gn/T//ompi.giulios-mbp.501/pid.4617/1/vader_segment.giulios-mbp.501.588a0001.13\n",
      "  Error:       No such file or directory (errno 2)\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!mpiexec --oversubscribe -n 15 python sol08pr01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending result back to Process 0\n",
      "Received Final Number From Process 19: 2470\n",
      "--------------------------------------------------------------------------\n",
      "A system call failed during shared memory initialization that should\n",
      "not have.  It is likely that your MPI job will now either abort or\n",
      "experience performance degradation.\n",
      "\n",
      "  Local host:  giulios-mbp.lan\n",
      "  System call: unlink(2) /var/folders/rk/rwsr6gss0vz3g4fz3_kt0x0m0000gn/T//ompi.giulios-mbp.501/pid.4636/1/vader_segment.giulios-mbp.501.589f0001.16\n",
      "  Error:       No such file or directory (errno 2)\n",
      "--------------------------------------------------------------------------\n",
      "[giulios-mbp.lan:04636] 1 more process has sent help message help-opal-shmem-mmap.txt / sys call fail\n",
      "[giulios-mbp.lan:04636] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n"
     ]
    }
   ],
   "source": [
    "!mpiexec --oversubscribe -n 20 python sol08pr01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending result back to Process 0\n",
      "Received Final Number From Process 24: 4900\n",
      "--------------------------------------------------------------------------\n",
      "A system call failed during shared memory initialization that should\n",
      "not have.  It is likely that your MPI job will now either abort or\n",
      "experience performance degradation.\n",
      "\n",
      "  Local host:  giulios-mbp.lan\n",
      "  System call: unlink(2) /var/folders/rk/rwsr6gss0vz3g4fz3_kt0x0m0000gn/T//ompi.giulios-mbp.501/pid.4657/1/vader_segment.giulios-mbp.501.58b20001.13\n",
      "  Error:       No such file or directory (errno 2)\n",
      "--------------------------------------------------------------------------\n",
      "[giulios-mbp.lan:04657] 8 more processes have sent help message help-opal-shmem-mmap.txt / sys call fail\n",
      "[giulios-mbp.lan:04657] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n"
     ]
    }
   ],
   "source": [
    "!mpiexec --oversubscribe -n 25 python sol08pr01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 2 (50p)**\n",
    "\n",
    "Write an MPI program that does the following. There are two processes 0 and 1 that have to exchange $T=10$ messages.  \n",
    "\n",
    "\n",
    "Process 0 initially reads two float variables from the standard input, call them $x, y$, and must ensure $x \\neq 0$ and $y \\neq 0$. For example this can be done as:\n",
    "\n",
    "```\n",
    "import sys\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    x = float(line)        \n",
    "    if x != 0.0:\n",
    "        break\n",
    "for line in sys.stdin:\n",
    "    y = float(line)        \n",
    "    if y != 0.0:\n",
    "        break\n",
    "```\n",
    "\n",
    "\n",
    "Both Process 0 and Process 1 will carry main results in an element that is part of a process buffer and called $p$. The value in $p$ is initially set to $1$. \n",
    "\n",
    "\n",
    "Now the exchange of messages is as follows.\n",
    "\n",
    "\n",
    "0. Message00: Process 0 multiplies its own value in $p$ by $x$ and sends the whole buffer to Process 1.\n",
    "\n",
    "1. Message01: Process 1 divides its own value in $p$ by $y$ and sends the whole buffer to Process 0.\n",
    "\n",
    "2. Message01: Process 0 multiplies its own value in $p$ by $x$ and sends the whole buffer to Process 1.\n",
    "\n",
    "3. Message02: Process 1 divides its own value in $p$ by $y$ and sends the whole buffer to Process 0.\n",
    "\n",
    "\n",
    "etc.\n",
    "\n",
    "8. Message08: Process 0 multiplies its own value in $p$ by $x$ and sends the whole buffer to Process 1.\n",
    "\n",
    "9. Message09: Process 1 divides its own value in $p$ by $y$ and sends the whole buffer to Process 0.\n",
    "\n",
    "Finally, Process 0 prints the value in $p$ as a final result. \n",
    "\n",
    "\n",
    "Write the code that implements the protocol above. Additionally, provide results for: $(x, y) = (2, 4)$, $(x, y) = (1, 3)$, $(x, y) = (5, 7)$ and $(x, y) = (5, 10)$.\n",
    "\n",
    "\n",
    "**Note**: You can use either blocking or non-blocking operations.Make sure to provide adequate comments and documentation in the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sol08pr02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sol08pr02.py\n",
    "#Imports\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import argparse\n",
    "\n",
    "# Initialize comm network, get rank so processes can identify themselves / execute as appropriate\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "info = MPI.Status()\n",
    "\n",
    "# Get total number of processes\n",
    "size = comm.Get_size() \n",
    "last_rank = size-1\n",
    "\n",
    "# Initialize buffer for each process\n",
    "p = np.ones(1)\n",
    "\n",
    "# Process 0 Logic\n",
    "if rank == 0:   \n",
    "    \n",
    "    # Using arg parse rather than sys.stdin \n",
    "    # Easier to prove code works in Jupyter Notebook / Run from CLI\n",
    "    parser = argparse.ArgumentParser(\n",
    "                    prog='sol08pr02',\n",
    "                    description='Solves Problem 2 as described',\n",
    "                    )\n",
    "\n",
    "    parser.add_argument('nums', type=float, nargs=2,\n",
    "                        help='stores 2 numbers, x and y')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Retrieve x,y from CLI\n",
    "    x,y = args.nums\n",
    "    \n",
    "    # Check for x,y != 0\n",
    "    if (x == 0.0) or (y == 0.0):\n",
    "        raise AssertionError(f\"Inputs must be non zero! x={x}, y={y}\")\n",
    "    \n",
    "    # Make values into arrays to send\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Send y to Process 1\n",
    "    comm.Send(y, dest=1)\n",
    "        \n",
    "    # Mulitply p * x, send to Process 1\n",
    "    for i in range(5):\n",
    "        p *= x\n",
    "        print(f\"Sending message {i}, p={p}\")\n",
    "        comm.Send(p, dest=1)\n",
    "        comm.Recv(p, source=1)\n",
    "    print(f\"Final Value of p={p[0]}\")\n",
    "    \n",
    "        \n",
    "# Process 1 Logic\n",
    "if rank == 1:\n",
    "    \n",
    "    # Receive y from Process 0\n",
    "    y = np.ones(1)\n",
    "    comm.Recv(y, source=0)\n",
    "    \n",
    "    # Receive p from Process 0\n",
    "    for i in range(1,10,2):\n",
    "        comm.Recv(p, source=0)\n",
    "        \n",
    "        # Divide p by y\n",
    "        p /= y\n",
    "        # Send back to Process 0\n",
    "        print(f\"Sending message {i}, p={p}\")\n",
    "        comm.Send(p,dest=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message 0, p=[2.]\n",
      "Sending message 1, p=[0.5]\n",
      "Sending message 1, p=[1.]\n",
      "Sending message 3, p=[0.25]\n",
      "Sending message 2, p=[0.5]\n",
      "Sending message 5, p=[0.125]\n",
      "Sending message 3, p=[0.25]\n",
      "Sending message 7, p=[0.0625]\n",
      "Sending message 4, p=[0.125]\n",
      "Sending message 9, p=[0.03125]\n",
      "Final Value of p=0.03125\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python sol08pr02.py 2 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message 0, p=[1.]\n",
      "Sending message 1, p=[0.33333333]\n",
      "Sending message 1, p=[0.33333333]\n",
      "Sending message 3, p=[0.11111111]\n",
      "Sending message 2, p=[0.11111111]\n",
      "Sending message 5, p=[0.03703704]\n",
      "Sending message 3, p=[0.03703704]\n",
      "Sending message 7, p=[0.01234568]\n",
      "Sending message 4, p=[0.01234568]\n",
      "Final Value of p=0.004115226337448559\n",
      "Sending message 9, p=[0.00411523]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python sol08pr02.py 1 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message 0, p=[5.]\n",
      "Sending message 1, p=[0.71428571]\n",
      "Sending message 1, p=[3.57142857]\n",
      "Sending message 3, p=[0.51020408]\n",
      "Sending message 2, p=[2.55102041]\n",
      "Sending message 3, p=[1.82215743]\n",
      "Sending message 5, p=[0.36443149]\n",
      "Sending message 7, p=[0.2603082]\n",
      "Sending message 4, p=[1.30154102]\n",
      "Sending message 9, p=[0.18593443]\n",
      "Final Value of p=0.1859344320818706\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python sol08pr02.py 5 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message 0, p=[5.]\n",
      "Sending message 1, p=[0.5]\n",
      "Sending message 1, p=[2.5]\n",
      "Sending message 3, p=[0.25]\n",
      "Sending message 5, p=[0.125]\n",
      "Sending message 2, p=[1.25]\n",
      "Sending message 3, p=[0.625]\n",
      "Sending message 7, p=[0.0625]\n",
      "Sending message 4, p=[0.3125]\n",
      "Final Value of p=0.03125\n",
      "Sending message 9, p=[0.03125]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python sol08pr02.py 5 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "05534ed6e750634128f4013d25855b1dc3ee83c62a29dea85067392c29148714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
