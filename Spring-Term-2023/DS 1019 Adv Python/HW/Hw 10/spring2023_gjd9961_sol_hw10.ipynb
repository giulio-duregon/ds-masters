{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 10: GPUs\n",
    "\n",
    "## Due Date: April 26, 2023, 11:59pm\n",
    "\n",
    "#### Firstname Lastname: Giulio Duregon\n",
    "\n",
    "#### E-mail: gjd9961@nyu.edu\n",
    "\n",
    "#### Enter your solutions and submit this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1 (100p)**\n",
    "\n",
    "\n",
    "Write two programs which will be able to run in parallel on a GPU, one using Numba/CUDA (50p), one using PyOpenCL (50p).\n",
    "\n",
    "\n",
    "Each program will:\n",
    "\n",
    "- draw two random vectors $\\vec u$ and $\\vec v$ from $[0,1]^N$ where $N = 10^7$;\n",
    "\n",
    "\n",
    "- calculate and output similarity between $\\vec u$ and $\\vec v$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The similarity between two vectors $\\vec u$ and $\\vec v$ is defined here as a `cosine` value of the angle between them $\\measuredangle \\left( \\vec u, \\vec v \\right)$. That is, the program returns: \n",
    "\n",
    "$$\\cos \\left( \\measuredangle \\left( \\vec u, \\vec v \\right) \\right).$$\n",
    "\n",
    "\n",
    "Note that the output is a real value and must belong to $[-1, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyOpenCL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 0.27574s\n",
      "Cosine Similarity Value: 0.75027\n"
     ]
    }
   ],
   "source": [
    "# the same above algorithm but written in a different way\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "\n",
    "np.random.seed(10)\n",
    "# Set start time / number of samples to draw\n",
    "num_samples = 10 ** 7\n",
    "start_time = time()\n",
    "\n",
    "# generate random vectors\n",
    "v, u  = np.random.rand(num_samples).astype(np.float32), np.random.rand(num_samples).astype(np.float32)\n",
    "\n",
    "# PyOpenCL setup\n",
    "ctx   = cl.create_some_context()\n",
    "q = cl.CommandQueue(ctx)\n",
    "memory_flags = cl.mem_flags\n",
    "\n",
    "# Initialize buffers to read in data to GPU\n",
    "u_buffer = cl.Buffer(ctx, memory_flags.READ_ONLY | memory_flags.COPY_HOST_PTR, hostbuf=u)\n",
    "v_buffer = cl.Buffer(ctx, memory_flags.READ_ONLY | memory_flags.COPY_HOST_PTR, hostbuf=v)\n",
    "\n",
    "# Add context to program (More PyOpenCL setup)\n",
    "prg = cl.Program(ctx, \"\"\"\n",
    "__kernel void fnct(\n",
    "__global const float *u_buffer, \n",
    "__global const float *v_buffer, \n",
    "__global float *u_norm_buffer,\n",
    "__global float *v_norm_buffer,\n",
    "__global float *dot_product_buffer){\n",
    "\n",
    "int gid = get_global_id(0);\n",
    "u_norm_buffer[gid] = u_buffer[gid] * u_buffer[gid];\n",
    "v_norm_buffer[gid] = v_buffer[gid] * v_buffer[gid];\n",
    "dot_product_buffer[gid] = u_buffer[gid] * v_buffer[gid];\n",
    "}\"\"\").build()\n",
    "\n",
    "# Build buffers for our vector dot products / norm calculuations\n",
    "u_norm_buffer = cl.Buffer(ctx, memory_flags.WRITE_ONLY, u.nbytes)\n",
    "v_norm_buffer = cl.Buffer(ctx, memory_flags.WRITE_ONLY, u.nbytes)\n",
    "dot_product_buffer = cl.Buffer(ctx, memory_flags.WRITE_ONLY, u.nbytes)\n",
    "\n",
    "# More PyOpenCL setup\n",
    "prg.fnct(q, u.shape, None, u_buffer, v_buffer, u_norm_buffer, v_norm_buffer, dot_product_buffer)\n",
    "\n",
    "# Initliaze empty np.arrays for results\n",
    "uv = np.empty_like(u)\n",
    "uu = np.empty_like(u)\n",
    "vv = np.empty_like(v)\n",
    "\n",
    "# Add arrays to queue\n",
    "cl.enqueue_copy(q, uu, u_norm_buffer)\n",
    "cl.enqueue_copy(q, vv, v_norm_buffer)\n",
    "cl.enqueue_copy(q, uv, dot_product_buffer)\n",
    "\n",
    "# Calculate Cosine Similarity as \n",
    "cosine_similarity = np.sum(uv) / (np.sqrt(np.sum(uu)) * np.sqrt(np.sum(vv)))\n",
    "\n",
    "# Print output\n",
    "print(f\"Run Time: {(time() - start_time):.05f}s\")\n",
    "print(f\"Cosine Similarity Value: {cosine_similarity:.05f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba + Cuda Implementation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As I don't have a GPU on my laptop\n",
    "## If testing with a GPU, comment out the two cells below\n",
    "\n",
    "When running on a google colab GPU results are as follows: \n",
    "- Total Time (s): 0.34177327156066895, Cosine Similarity: 0.7498810839929098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NUMBA_ENABLE_CUDASIM=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NUMBA_ENABLE_CUDASIM=1\n"
     ]
    }
   ],
   "source": [
    "%env NUMBA_ENABLE_CUDASIM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "print(cuda.gpus)\n",
    "cuda.select_device(0)\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def cosine_similarity_gpu(u, v,uu ,vv, uv ,res):\n",
    "    # Get the global id of the thread\n",
    "    x = cuda.grid(1)\n",
    "    if x >= u.shape[0]:\n",
    "        return\n",
    "    \n",
    "    # Compute the dot product between a and b\n",
    "    # Compute the norm of a and b\n",
    "    cuda.atomic.add(uu, 0, u[x]*u[x])\n",
    "    cuda.atomic.add(uv, 0, u[x]*v[x])\n",
    "    cuda.atomic.add(vv, 0, v[x]*v[x])\n",
    "\n",
    "    # Wait for threads to be done\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    if x == 0:\n",
    "        res[0] = uv[0] / (math.sqrt(uu[0]) * math.sqrt(vv[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "print(cuda.gpus)\n",
    "cuda.select_device(0)\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def cosine_similarity_gpu(u, v,uu ,vv, uv ,res):\n",
    "    # Get the global id of the thread\n",
    "    x = cuda.grid(1)\n",
    "    if x >= u.shape[0]:\n",
    "        return\n",
    "    \n",
    "    # Compute the dot product between a and b\n",
    "    # Compute the norm of a and b\n",
    "    cuda.atomic.add(uu, 0, u[x]*u[x])\n",
    "    cuda.atomic.add(uv, 0, u[x]*v[x])\n",
    "    cuda.atomic.add(vv, 0, v[x]*v[x])\n",
    "\n",
    "    # Wait for threads to be done\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    res[0] = uv[0] / (math.sqrt(uu[0]) * math.sqrt(vv[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m blocks_per_grid \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mceil(u\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m TPB))\n\u001b[1;32m     22\u001b[0m \u001b[39m# Run our function, output result\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m cosine_similarity_gpu[blocks_per_grid, TPB](u, v, uu, vv, uv, res)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal Time (s): \u001b[39m\u001b[39m{\u001b[39;00mtime()\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m}\u001b[39;00m\u001b[39m, Cosine Similarity: \u001b[39m\u001b[39m{\u001b[39;00mres[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hw_8/lib/python3.9/site-packages/numba/cuda/simulator/kernel.py:122\u001b[0m, in \u001b[0;36mFakeCUDAKernel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[39mfor\u001b[39;00m grid_point \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mndindex(\u001b[39m*\u001b[39mgrid_dim):\n\u001b[1;32m    121\u001b[0m         bm \u001b[39m=\u001b[39m BlockManager(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, grid_dim, block_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debug)\n\u001b[0;32m--> 122\u001b[0m         bm\u001b[39m.\u001b[39;49mrun(grid_point, \u001b[39m*\u001b[39;49mfake_args)\n\u001b[1;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m wb \u001b[39min\u001b[39;00m retr:\n\u001b[1;32m    125\u001b[0m     wb()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hw_8/lib/python3.9/site-packages/numba/cuda/simulator/kernel.py:302\u001b[0m, in \u001b[0;36mBlockManager.run\u001b[0;34m(self, grid_point, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             t\u001b[39m.\u001b[39msyncthreads_event\u001b[39m.\u001b[39mset()\n\u001b[1;32m    301\u001b[0m         blockedthreads \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m--> 302\u001b[0m     livethreads \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([ t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m livethreads \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_alive() ])\n\u001b[1;32m    303\u001b[0m \u001b[39m# Final check for exceptions in case any were set prior to thread\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# finishing, before we could check it\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m threads:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hw_8/lib/python3.9/site-packages/numba/cuda/simulator/kernel.py:302\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    300\u001b[0m             t\u001b[39m.\u001b[39msyncthreads_event\u001b[39m.\u001b[39mset()\n\u001b[1;32m    301\u001b[0m         blockedthreads \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m--> 302\u001b[0m     livethreads \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([ t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m livethreads \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39;49mis_alive() ])\n\u001b[1;32m    303\u001b[0m \u001b[39m# Final check for exceptions in case any were set prior to thread\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# finishing, before we could check it\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m threads:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hw_8/lib/python3.9/threading.py:1144\u001b[0m, in \u001b[0;36mThread.is_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_started\u001b[39m.\u001b[39mis_set():\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1144\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock(\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/hw_8/lib/python3.9/threading.py:1079\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n\u001b[1;32m   1077\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1080\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1081\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# Init start time\n",
    "start_time = time()\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "TPB = 16\n",
    "\n",
    "# Initliaze 2* 10^7 randomly drawn numbers for our vectors\n",
    "num_samples = 10 ** 7\n",
    "v, u  = np.random.rand(num_samples).astype(np.float32), np.random.rand(num_samples).astype(np.float32)\n",
    "\n",
    "# Buffer for temp results\n",
    "vv = np.zeros(1)\n",
    "uv = np.zeros(1)\n",
    "uu = np.zeros(1)\n",
    "res = np.zeros(1, float)\n",
    "\n",
    "# Calculate blocks per brid\n",
    "blocks_per_grid = int(math.ceil(u.shape[0] / TPB))\n",
    "\n",
    "# Run our function, output result\n",
    "\n",
    "cosine_similarity_gpu[blocks_per_grid, TPB](u, v, uu, vv, uv, res)\n",
    "\n",
    "print(f\"Total Time (s): {time()- start_time}, Cosine Similarity: {res[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "05534ed6e750634128f4013d25855b1dc3ee83c62a29dea85067392c29148714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
